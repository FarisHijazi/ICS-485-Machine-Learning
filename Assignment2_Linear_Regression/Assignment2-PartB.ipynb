{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will create a **gradient descent** solver for **ridge regression** and **lasso regression** then compare it to the built-in libraries in `sklearn.linear_model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up notebook and load data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading in some standard packages, we load synthetic data set consisting of data points `(x,y)`:\n",
    "* `x`: 100-dimensional vector whose coordinates are independent draws from a standard normal (Gaussian) distribution\n",
    "* `y`: response value given by `y = wx + e` where `w` is a target regression function and `e` is Gaussian noise. **`y` was generated using only 10 of the 100 features (which are unknown to you, at least for now!).**}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next snippet of code loads in the dataset. There are 100 data points for train and test set respectively, each with 100 predictor variables (which we'll denote `x`) and one response variable (which we'll denote `y`).\n",
    "\n",
    "Make sure the files `'trainx.csv'`,`'trainy.csv'`,`'testx.csv'`, and `'testy.csv'` are in the same directory as this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = np.genfromtxt('trainx.csv', delimiter=',')\n",
    "trainy = np.genfromtxt('trainy.csv', delimiter=',')\n",
    "testx = np.genfromtxt('testx.csv', delimiter=',')\n",
    "testy = np.genfromtxt('testy.csv', delimiter=',')\n",
    "trainx.shape, trainy.shape,testx.shape,testy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient descent solver for ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\">**For you to do:**</font> Define a procedure, **ridge_regression_GD**, that uses gradient descent to solve the ridge regression problem. It is invoked as follows:\n",
    "\n",
    "* `w,b,losses = ridge_regression_GD(x,y,C)`\n",
    "\n",
    "Here, the input consists of:\n",
    "* training data `trainx, trainy`, where `trainx` and `trainy` are numpy arrays of dimension `m`-by-`n` and `m`, respectively (if there are `m` training points and `n` features)\n",
    "* regularization constant `C`, we normally use the term **lambda**.\n",
    "\n",
    "The function should find the `n`-dimensional vector `w` and offset `b` that minimize the MSE loss function (with regularization constant `C`), and return:\n",
    "* `w` and `b`\n",
    "* `losses`, an array containing the MSE loss at each iteration\n",
    "\n",
    "<font color=\"magenta\">Advice:</font> First figure out the derivative, which has a relatively simple form. Next, when implementing gradient descent, think carefully about two issues.\n",
    "\n",
    "1. What is the step size (learning rate)?\n",
    "2. When has the procedure converged?\n",
    "\n",
    "Take the time to experiment with different ways of handling these.\n",
    "\n",
    "**Note:** You can use additional methods as helpers if you feel the need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_GD(x,y,C):\n",
    "    ### START CODE HERE ###\n",
    "    return w,b,losses\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out and print a graph of the loss values during the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set regularization constant\n",
    "C = 1.0 # you can try different values for C\n",
    "# Run gradient descent solver\n",
    "w, b, losses = ridge_regression_GD(trainx,trainy,C)\n",
    "# Plot the losses\n",
    "plt.plot(losses,'r')\n",
    "plt.xlabel('Iterations', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate the gradient descent solver for ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the regressor found by your gradient descent procedure to that returned by the built-in ridge regression solver in `sklearn`. We will compare them by their resulting MSE values. We will also compare the results of the built-in linear regression (without regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code to compute the MSE value given `w`, `b`, `x`, and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(w,b,x,y):\n",
    "    ### START CODE HERE ###\n",
    "    return None\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set regularization constant\n",
    "C = 1.0 # you can change it\n",
    "# Run gradient descent solver and compute its MSE\n",
    "w, b, losses = ridge_regression_GD(trainx, trainy, C)\n",
    "# Use built-in routine for linear regression and compute MSE\n",
    "lin_regr = linear_model.LinearRegression()\n",
    "lin_regr.fit(trainx, trainy)\n",
    "# Use built-in routine for ridge regression and compute MSE\n",
    "ridge_regr = linear_model.Ridge(alpha=1.0) # you can try different values\n",
    "ridge_regr.fit(trainx, trainy)\n",
    "# Print MSE values\n",
    "print \"MSE of built-in linear regression(training): \", mean_squared_error(lin_regr.predict(trainx), trainy)\n",
    "print \"MSE of gradient descent solver for ridge regression (training): \", compute_mse(w,b,trainx, trainy)\n",
    "print \"MSE of built-in solver for ridge regression (training): \", mean_squared_error(ridge_regr.predict(trainx), trainy)\n",
    "print \"MSE of built-in linear regression(test): \", mean_squared_error(lin_regr.predict(testx), testy)\n",
    "print \"MSE of gradient descent solver for ridge regression (test): \", compute_mse(w,b,testx, testy)\n",
    "print \"MSE of built-in solver for ridge regression (test): \", mean_squared_error(ridge_regr.predict(testx), testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient descent solver for lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\">**For you to do:**</font> Define a procedure, **lasso_regression_GD**, that uses gradient descent to solve the lasso regression problem. It is invoked as follows:\n",
    "\n",
    "* `w,b,losses = lasso_regression_GD(x,y,C)`\n",
    "\n",
    "Here, the input consists of:\n",
    "* training data `trainx, trainy`, where `trainx` and `trainy` are numpy arrays of dimension `m`-by-`n` and `m`, respectively (if there are `m` training points and `n` features)\n",
    "* regularization constant `C`, we normally use the term **lambda**.\n",
    "\n",
    "The function should find the `n`-dimensional vector `w` and offset `b` that minimize the MSE loss function (with regularization constant `C`), and return:\n",
    "* `w` and `b`\n",
    "* `losses`, an array containing the MSE loss at each iteration\n",
    "\n",
    "<font color=\"magenta\">Advice:</font> First figure out the derivative, which has a relatively simple form. Next, when implementing gradient descent, think carefully about two issues.\n",
    "\n",
    "1. What is the step size (learning rate)?\n",
    "2. When has the procedure converged?\n",
    "\n",
    "Take the time to experiment with different ways of handling these.\n",
    "\n",
    "**Note:** You can use additional methods as helpers if you feel the need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regression_GD(x,y,C):\n",
    "    ### START CODE HERE ###\n",
    "    return w,b,losses\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out and print a graph of the loss values during the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set regularization constant\n",
    "C = 1.0 # you can try different values for C\n",
    "# Run gradient descent solver\n",
    "w, b, losses = lasso_regression_GD(trainx,trainy,C)\n",
    "# Plot the losses\n",
    "plt.plot(losses,'r')\n",
    "plt.xlabel('Iterations', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the gradient descent solver for lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the regressor found by your gradient descent procedure to that returned by the built-in ridge regression solver in `sklearn`. We will compare them by their resulting MSE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set regularization constant\n",
    "C = 1.0 # you can change it\n",
    "# Run gradient descent solver and compute its MSE\n",
    "w, b, losses = lasso_regression_GD(trainx,trainy,C)\n",
    "# Use built-in routine for ridge regression and compute MSE\n",
    "lasso_regr = linear_model.Lasso(alpha=1.0) # you can try different values\n",
    "lasso_regr.fit(trainx,trainy)\n",
    "# Print MSE values\n",
    "print \"MSE of built-in linear regression(training): \", mean_squared_error(lin_regr.predict(trainx), trainy)\n",
    "print \"MSE of gradient descent solver for lasso regression (training): \", compute_mse(w,b,trainx, trainy)\n",
    "print \"MSE of built-in solver for lasso regression (training): \", mean_squared_error(lasso_regr.predict(trainx), trainy)\n",
    "print \"MSE of built-in linear regression(test): \", mean_squared_error(lin_regr.predict(testx), testy)\n",
    "print \"MSE of gradient descent solver for lasso regression (test): \", compute_mse(w,b,testx, testy)\n",
    "print \"MSE of built-in solver for lasso regression (test): \", mean_squared_error(lasso_regr.predict(testx), testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Documents all the results in the report.\n",
    "2. Try with a large value of C (e.g., 20) for lasso and chaeck teh weights and MSE. What do you observe?\n",
    "3. Compare the coeffecients (parameter values) for ridge and lasso for the best setup. What do you observe? Can you explain?\n",
    "4. Compare MSE of linear, ridge, and lasso. What do you observe?\n",
    "5. Which among the ridge and lasso gives teh best results on the test? Can you explain why?\n",
    "6. Can the lasso regression retreive the 10 features which were used in the equation for y? List them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
