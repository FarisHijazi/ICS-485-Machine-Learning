{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the progression of diabetes using least-squares regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **diabetes** data set is provided as a single file, `diabetes-data.csv`. We obtained it at https://web.stanford.edu/~hastie/Papers/LARS/diabetes.data. For some background information on the data, see this seminal paper:\n",
    "\n",
    "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up notebook and load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard includes\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Routines for linear regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Set label size for plots\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next snippet of code loads in the diabetes data. There are 442 data points, each with 10 predictor variables (which we'll denote `x`) and one response variable (which we'll denote `y`).\n",
    "\n",
    "Make sure the file `'diabetes-data.csv'` is in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('diabetes-data.csv', delimiter=',')\n",
    "features = ['age', 'sex', 'body mass index', 'blood pressure', \n",
    "            'serum1', 'serum2', 'serum3', 'serum4', 'serum5', 'serum6']\n",
    "x = data[:,0:10] # predictors\n",
    "y = data[:,10] # response variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict `y` without using `x`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to predict `y` without knowledge of `x`, what value would be predict? The <font color=\"magenta\">mean</font> value of `y`.\n",
    "\n",
    "In this case, the mean squared error (MSE) associated with the prediction is simply the variance of `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Prediction: \", np.mean(y))\n",
    "print (\"Mean squared error: \", np.var(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict `y` using a single feature of `x`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit a linear regression model, we could directly use the formula we saw in lecture. To make things even easier, this is already implemented in `sklearn.linear_model.LinearRegression()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function, **one_feature_regression**, that takes `x` and `y`, along with the index `f` of a single feature and fits a linear regressor to `(x[f],y)`. It then plots the data along with the resulting line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_feature_regression(x,y,f):\n",
    "    if (f < 0) or (f > 9):\n",
    "        print (\"Feature index is out of bounds\")\n",
    "        return\n",
    "    regr = linear_model.LinearRegression()\n",
    "    x1 = x[:,[f]]\n",
    "    regr.fit(x1, y)\n",
    "    # Make predictions using the model\n",
    "    y_pred = regr.predict(x1)\n",
    "    # Plot data points as well as predictions\n",
    "    plt.plot(x1, y, 'bo')\n",
    "    plt.plot(x1, y_pred, 'r-', linewidth=3)\n",
    "    plt.xlabel(features[f], fontsize=14)\n",
    "    plt.ylabel('Progression of disease', fontsize=14)\n",
    "    plt.show()\n",
    "    print (\"Mean squared error: \", mean_squared_error(y, y_pred))\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this with feature #2 (body mass index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = one_feature_regression(x,y,2)\n",
    "print (\"w = \", regr.coef_)\n",
    "print (\"b = \", regr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\">For you to try:</font> Feature #2 ('body mass index') is the single feature that yields the lowest mean squared error. Which feature is the second best? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can use this space to figure out the second-best feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict `y` using a specified subset of features from `x`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **feature_subset_regression** is just like **one_feature_regression**, but this time uses a list of features `flist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_subset_regression(x,y,flist):\n",
    "    if len(flist) < 1:\n",
    "        print (\"Need at least one feature\")\n",
    "        return\n",
    "    for f in flist:\n",
    "        if (f < 0) or (f > 9):\n",
    "            print (\"Feature index is out of bounds\")\n",
    "            return\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x[:,flist], y)\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using just features #2 (body mass index) and #8 (serum5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = [2,8]\n",
    "regr = feature_subset_regression(x,y,[2,8])\n",
    "print (\"w = \", regr.coef_)\n",
    "print (\"b = \", regr.intercept_)\n",
    "print (\"Mean squared error: \", mean_squared_error(y, regr.predict(x[:,flist])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use all 10 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = feature_subset_regression(x,y,range(0,10))\n",
    "print (\"w = \", regr.coef_)\n",
    "print (\"b = \", regr.intercept_)\n",
    "print (\"Mean squared error: \", mean_squared_error(y, regr.predict(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Splitting the data into a training and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a procedure **split_data** that partitions the data set into separate training and test sets. It is invoked as follows:\n",
    "\n",
    "* `trainx, trainy, testx, testy = split_data(n_train)`\n",
    "\n",
    "Here:\n",
    "* `n_train` is the desired number of training points\n",
    "* `trainx` and `trainy` are the training points and response values\n",
    "* `testx` and `testy` are the test points and response values\n",
    "\n",
    "The split is done randomly, but the random seed is fixed, and thus the same split is produced if the procedure is called repeatedly with the same `n_train` parameter.\n",
    "**Note:** You can also use python built-in libraries for splitting data like:\n",
    "`from sklearn.model_selection import train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(n_train):\n",
    "    if (n_train < 0) or (n_train > 442):\n",
    "        print (\"Invalid number of training points\")\n",
    "        return\n",
    "    np.random.seed(0)\n",
    "    perm = np.random.permutation(442)\n",
    "    training_indices = perm[range(0,n_train)]\n",
    "    test_indices = perm[range(n_train,442)]\n",
    "    trainx = x[training_indices,:]\n",
    "    trainy = y[training_indices]\n",
    "    testx = x[test_indices,:]\n",
    "    testy = y[test_indices]\n",
    "    return trainx, trainy, testx, testy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementing the closed-form solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit a linear regression model, we can directly use the closed-form formula we saw in lecture. Implement a method to get the parameters of the linear regression using the closed-form solution. The method should take features `x` and predictions `y` of the training set and return back the parameter values including the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_CF(trainx, trainy):\n",
    "    # inputs: trainx and trainy, the features and the target in the training set\n",
    "    # output: a vector of weights including the bias term\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    return None\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing the iterative solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you you are required to implement the iterative (gradient descent) solution. The method should take features `x` and predictions `y` of the training set and return back the parameter values including the bias term. You should also initialize the hyper-parameters in the beginning of the method. Also, plot the the cost function at different iterations.\n",
    "Here, the input consists of:\n",
    "* training data `trainx, trainy`, where `trainx` and `trainy` are numpy arrays of dimension `m`-by-`n` and `m`, respectively (if there are `m` training points and `n` features)\n",
    "\n",
    "The function should find the `n`-dimensional vector `w` and offset `b` that minimize the MSE loss function, and return:\n",
    "* `w` and `b`\n",
    "* `losses`, an array containing the MSE loss at each iteration\n",
    "\n",
    "<font color=\"magenta\">Advice:</font> First figure out the derivative, which has a relatively simple form. Next, when implementing gradient descent, think carefully about two issues.\n",
    "\n",
    "1. What is the step size (learning rate)?\n",
    "2. When has the procedure converged?\n",
    "\n",
    "Take the time to experiment with different ways of handling these.\n",
    "\n",
    "**Note:** You can use additional methods as helpers if you feel the need.\n",
    "\n",
    "**Note:** MSE is the RSS value divided by the number of samples to get the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_GD(trainx, trainy):\n",
    "    # inputs: trainx and trainy, the features and the target in the training set\n",
    "    # output: a vector of weights including the bias term\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    return None\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use diffrent amounts of training data to fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the **split_data** procedure to partition the data set, compute the training MSE and test MSE when fitting a regressor to *all* features, for the following training set sizes:\n",
    "* `n_train = 20`\n",
    "* `n_train = 50`\n",
    "* `n_train = 100`\n",
    "* `n_train = 200`\n",
    "* `n_train = 300`\n",
    "\n",
    "\n",
    "1. Compare your results for the three approaches, i.e., using library, using closed-form solution, and using the iterative solution. Provide your comments on the results.\n",
    "2. Compare the parameter values for the three solutions when using `n_train = 300` training samples.\n",
    "\n",
    "**Analytical Questions:**\n",
    "\n",
    "3. What changes you need to do if the unit of `y` is different?\n",
    "4. What changes you need to do if the unit of one of the features was different? For example if **age** was in months and not in years. \n",
    "5. What if both 3 and 4 apply?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
