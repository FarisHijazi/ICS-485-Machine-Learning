{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRIS classification with the bivariate Gaussian\n",
    "\n",
    "Our first generative model for IRIS classification used just one feature. Now we use two features, modeling each class by a **bivariate Gaussian**.\n",
    "\n",
    "**Note:** You can use built-in code for mean, variance, covariance, determinant, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the univariate case, we start by loading in the IRIS data set.\n",
    "Recall that there are 150 data points, each with 4 features and a label (0,1,2). As before, we will divide this into a training set of 105 points and a test set of 45 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard includes\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Useful module for dealing with the Gaussian density\n",
    "from scipy.stats import norm, multivariate_normal  #in case you use buit-in library\n",
    "# installing packages for interactive graphs\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "featurenames = ['petal_length', 'petal_width', 'sepal_length', 'sepal_width']\n",
    "\n",
    "# Split 150 instances into training set (trainx, trainy) of size 105 and test set (testx, testy) of size 45\n",
    "np.random.seed(0)\n",
    "perm = np.random.permutation(150)\n",
    "trainx = X[perm[0:105],:]\n",
    "trainy = Y[perm[0:105]]\n",
    "testx = X[perm[105:150],:]\n",
    "testy = Y[perm[105:150]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = sum(trainy==0)\n",
    "c1 = sum(trainy==1)\n",
    "c2 = sum(trainy==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class0 = trainx[trainy == 0, :]\n",
    "Class1 = trainx[trainy == 1, :]\n",
    "Class2 = trainx[trainy == 2, :]\n",
    "\n",
    "        \n",
    "Class0_mu = Class0.mean(axis = 0)\n",
    "Class0_var = Class0.var(axis = 0)\n",
    "Class0_pi = c0 / len(trainy)\n",
    "\n",
    "Class1_mu = Class1.mean(axis = 0)\n",
    "Class1_var = Class1.var(axis = 0)\n",
    "Class1_pi = c1 / len(trainy)\n",
    "\n",
    "Class2_mu = Class2.mean(axis = 0)\n",
    "Class2_var = Class2.var(axis = 0)\n",
    "Class2_pi = c2 / len(trainy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.95757576, 3.43030303, 1.46666667, 0.25151515])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Class0_mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Look at the distribution of two features from one of the species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to plot the distribution of two features for a particular species. We will use several helper functions for this. It is worth understanding each of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first helper function fits a Gaussian to a data set, restricting attention to specified features.\n",
    "It returns the mean and covariance matrix of the Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Gaussian to a data set using the selected features\n",
    "def fit_gaussian(x, features):\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    mu = x.mean(axis = 0)[features]\n",
    "   \n",
    "    covar = np.cov(x[:,features], rowvar = False)\n",
    "    ### END CODE HERE ###\n",
    "    return mu, covar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's look at the Gaussian we get for species-0, using features 0 ('petal_length') and 1 ('petal_width')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      "[4.95757576 3.43030303]\n",
      "Covariance matrix:\n",
      "[[0.14314394 0.12413826]\n",
      " [0.12413826 0.17030303]]\n"
     ]
    }
   ],
   "source": [
    "f1 = 0\n",
    "f2 = 1\n",
    "label = 0\n",
    "mu, covar = fit_gaussian(trainx[trainy==label,:], [f1,f2])\n",
    "print (\"Mean:\\n\" + str(mu))\n",
    "print (\"Covariance matrix:\\n\" + str(covar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will construct a routine for displaying points sampled from a two-dimensional Gaussian, as well as a few contour lines. Part of doing this involves deciding what range to use for each axis. We begin with a little helper function that takes as input an array of numbers (values along a single feature) and returns the range in which these numbers lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the range within which an array of numbers lie, with a little buffer\n",
    "def find_range(x):\n",
    "    lower = min(x)\n",
    "    upper = max(x)\n",
    "    width = upper - lower\n",
    "    lower = lower - 0.2 * width\n",
    "    upper = upper + 0.2 * width\n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a routine that plots a few contour lines of a given two-dimensional Gaussian.\n",
    "It takes as input:\n",
    "* `mu`, `cov`: the parameters of the Gaussian\n",
    "* `x1g`, `x2g`: the grid (along the two axes) at which the density is to be computed\n",
    "* `col`: the color of the contour lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(mu, cov, x1g, x2g, col):\n",
    "    rv = multivariate_normal(mean=mu, cov=cov)\n",
    "    z = np.zeros((len(x1g),len(x2g)))\n",
    "    for i in range(0,len(x1g)):\n",
    "        for j in range(0,len(x2g)):\n",
    "            z[j,i] = rv.logpdf([x1g[i], x2g[j]]) \n",
    "    sign, logdet = np.linalg.slogdet(cov)\n",
    "    normalizer = -0.5 * (2 * np.log(6.28) + sign * logdet)\n",
    "    for offset in range(1,4):\n",
    "        plt.contour(x1g,x2g,z, levels=[normalizer - offset], colors=col, linewidths=2.0, linestyles='solid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **two_features_plot** takes an input two features and a label, and displays the distribution for the specified species and pair of features.\n",
    "\n",
    "The first line allows you to specify the parameters interactively using sliders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d7dcbb7edd49c099663fa62950d1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='f1', max=3), IntSlider(value=1, description='f2', max=3)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual( f1=IntSlider(0,0,3,1), f2=IntSlider(1,0,3,1), label=IntSlider(0,0,2,1) )\n",
    "def two_features_plot(f1,f2,label):\n",
    "    if f1 == f2: # we need f1 != f2\n",
    "        print (\"Please choose different features for f1 and f2.\")\n",
    "        return  \n",
    "    \n",
    "    # Set up plot\n",
    "    x1_lower, x1_upper = find_range(trainx[trainy==label,f1])\n",
    "    x2_lower, x2_upper = find_range(trainx[trainy==label,f2])\n",
    "    plt.xlim(x1_lower, x1_upper) # limit along x1-axis\n",
    "    plt.ylim(x2_lower, x2_upper) # limit along x2-axis\n",
    "    \n",
    "    # Plot the training points along the two selected features\n",
    "    plt.plot(trainx[trainy==label, f1], trainx[trainy==label, f2], 'ro')\n",
    "\n",
    "    # Define a grid along each axis; the density will be computed at each grid point\n",
    "    res = 200 # resolution\n",
    "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
    "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
    "\n",
    "    # Now plot a few contour lines of the density\n",
    "    ### UPDATE CODE HERE ###\n",
    "    features = [f1, f2]\n",
    "    mu, cov = fit_gaussian(trainx[trainy == label,:], features)\n",
    "    plot_contours(mu, cov, x1g, x2g, 'k')\n",
    "    \n",
    "    # Finally, display\n",
    "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
    "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
    "    plt.title('Species-' + str(label), fontsize=14, color='blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fit a Gaussian to each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function that will fit a Gaussian generative model to the three classes, restricted to a given list of features. The function returns:\n",
    "* `mu`: the means of the Gaussians, one per row\n",
    "* `covar`: covariance matrices of each of the Gaussians\n",
    "* `pi`: list of three class weights summing to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes y takes on values 1,2,3\n",
    "def fit_generative_model(x, y, features):\n",
    "    k = 3 # number of classes\n",
    "    d = len(features) # number of features\n",
    "    mu = np.zeros((k,d)) # list of means  --It was k+1\n",
    "    covar = np.zeros((k,d,d)) # list of covariance matrices  --It was k+1\n",
    "    pi = np.zeros(k) # list of class weights  --It was k+1\n",
    "    for label in range(0,k):\n",
    "        indices = (y==label)\n",
    "        ### UPDATE CODE HERE ###\n",
    "        labelClass = x[y == label, :]\n",
    "        mu[label,:], covar[label,:,:] = labelClass.mean(axis = 0)[features], np.cov(labelClass[:,features], rowvar = False)\n",
    "        pi[label] = len(labelClass)/len(y) # the prior\n",
    "    return mu, covar, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the three Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10f0902260f4f70ac7d312d001a541a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='f1', max=3), IntSlider(value=1, description='f2', max=3)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual( f1=IntSlider(0,0,3,1), f2=IntSlider(1,0,3,1) )\n",
    "def three_class_plot(f1,f2):\n",
    "    if f1 == f2: # we need f1 != f2\n",
    "        print (\"Please choose different features for f1 and f2.\")\n",
    "        return  \n",
    "    \n",
    "    # Set up plot\n",
    "    x1_lower, x1_upper = find_range(trainx[:,f1])\n",
    "    x2_lower, x2_upper = find_range(trainx[:,f2])\n",
    "    plt.xlim(x1_lower, x1_upper) # limit along x1-axis\n",
    "    plt.ylim(x2_lower, x2_upper) # limit along x2-axis\n",
    "    \n",
    "    # Plot the training points along the two selected features\n",
    "    colors = ['r', 'y', 'g']\n",
    "    for label in range(0,3):\n",
    "        plt.plot(trainx[trainy==label,f1], trainx[trainy==label,f2], marker='o', ls='None', c=colors[label-1])\n",
    "\n",
    "    # Define a grid along each axis; the density will be computed at each grid point\n",
    "    res = 200 # resolution\n",
    "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
    "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
    "\n",
    "    # Show the Gaussian fit to each class, using features f1,f2\n",
    "    ### UPDATE CODE HERE ###\n",
    "    features = [f1, f2]\n",
    "    mu, covar, pi = fit_generative_model(trainx, trainy, features)\n",
    "    for label in range(0,3):\n",
    "        gmean = mu[label,:]\n",
    "        gcov = covar[label,:,:]\n",
    "        plot_contours(gmean, gcov, x1g, x2g, colors[label-1])\n",
    "\n",
    "    # Finally, display\n",
    "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
    "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
    "    plt.title('IRIS data', fontsize=14, color='blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict labels for the test points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well we can predict the class (1,2,3) based just on these two features?\n",
    "\n",
    "We start with a testing procedure that is analogous to what we developed in the 1-d case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalPDF(x, mu, covar, pi):\n",
    "    expPart = -0.5 * np.matmul(np.matmul(np.transpose(x - mu), np.linalg.inv(covar)), x-mu)\n",
    "    consPart = 1/(2*np.pi* np.sqrt(np.linalg.det(covar)))\n",
    "    res = pi * consPart * np.exp(expPart)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dea6de5e42447bf9e815c69e23b24f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='f1', max=3), IntSlider(value=1, description='f2', max=3)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now test the performance of a predictor based on a subset of features\n",
    "@interact( f1=IntSlider(0,0,3,1), f2=IntSlider(1,0,3,1) )\n",
    "def test_model(f1, f2):\n",
    "    if f1 == f2: # need f1 != f2\n",
    "        print (\"Please choose different features for f1 and f2.\")\n",
    "        return  \n",
    "    features= [f1,f2]\n",
    "    ### UPDATE CODE HERE ###\n",
    "    mu, covar, pi = fit_generative_model(trainx, trainy, features)\n",
    "    \n",
    "    k = 3 # Labels 1,2,...,k\n",
    "    nt = len(testy) # Number of test points\n",
    "    score = np.zeros((nt,k+1))\n",
    "    for i in range(0,nt):\n",
    "        for label in range(0,k):\n",
    "            ### START CODE HERE ###\n",
    "            # Implement the formula for normal pdf. \n",
    "            # If you can't, use the built-in multivariate_normal.logpdf but to get the full grades you should implement your own \n",
    "            score[i,label] = NormalPDF(testx[i][features], mu[label], covar[label], pi[label])\n",
    "    predictions = np.argmax(score, axis = 1)\n",
    "    ### END CODE HERE ###\n",
    "    # Finally, tally up score\n",
    "    errors = np.sum(predictions != testy)\n",
    "    print (\"Test error using feature(s): \")\n",
    "    for f in features:\n",
    "        print (\"'\" + featurenames[f] + \"'\" + \" \")\n",
    "    print\n",
    "    print (\"Errors: \" + str(errors) + \"/\" + str(nt))# Now test the performance of a predictor based on a subset of features\n",
    "    print(\"Error Rate features(%d, %d) = %.2f%%\"%(f1, f2, 100*errors / nt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qusetions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different pairs of features yield different test errors.\n",
    "1. What is the smallest achievable test error?\n",
    "2. Which pair of features achieves this minimum test error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. The decision boundary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **show_decision_boundary** takes as input two features, builds a classifier based only on these two features, and shows a plot that contains both the training data and the decision boundary.\n",
    "\n",
    "To compute the decision boundary, a dense grid is defined on the two-dimensional input space and the classifier is applied to every grid point. The built-in `pyplot.contour` function can then be invoked to depict the boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_decision_boundary(f1,f2):\n",
    "    # Fit Gaussian to each class\n",
    "    ### UPDATE CODE HERE ###\n",
    "    features = [f1, f2]\n",
    "    mu, covar, pi = fit_generative_model(trainx, trainy, features)\n",
    "    \n",
    "    # Set up dimensions of plot\n",
    "    x1_lower, x1_upper = find_range(trainx[:,f1])\n",
    "    x2_lower, x2_upper = find_range(trainx[:,f2])\n",
    "    plt.xlim([x1_lower,x1_upper])\n",
    "    plt.ylim([x2_lower,x2_upper])\n",
    "\n",
    "    # Plot points in training set\n",
    "    colors = ['r', 'k', 'g']\n",
    "    for label in range(0,3):\n",
    "        plt.plot(trainx[trainy==label,f1], trainx[trainy==label,f2], marker='o', ls='None', c=colors[label])\n",
    "\n",
    "    # Define a dense grid; every point in the grid will be classified according to the generative model\n",
    "    res = 200\n",
    "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
    "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
    "\n",
    "    # Declare random variables corresponding to each class density\n",
    "    random_vars = {}\n",
    "    for label in range(0,3):\n",
    "        random_vars[label] = multivariate_normal(mean=mu[label,:],cov=covar[label,:,:])\n",
    "\n",
    "    # Classify every point in the grid; these are stored in an array Z[]\n",
    "    Z = np.zeros((len(x1g), len(x2g)))\n",
    "    for i in range(0,len(x1g)):\n",
    "        for j in range(0,len(x2g)):\n",
    "            scores = []\n",
    "            for label in range(0,3):\n",
    "                scores.append(np.log(pi[label]) + random_vars[label].logpdf([x1g[i],x2g[j]]))\n",
    "            Z[i,j] = np.argmax(scores) + 1\n",
    "\n",
    "    # Plot the contour lines\n",
    "    plt.contour(x1g,x2g,Z.T,3,cmap='seismic')\n",
    "    \n",
    "    # Finally, show the image\n",
    "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
    "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the function above to draw the decision boundary using features 0 ('alcohol') and 6 ('flavanoids')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c239415f6244f6a39f7517ee8005c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='f1', max=3), IntSlider(value=1, description='f2', max=3)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual( f1=IntSlider(0,0,3,1), f2=IntSlider(1,0,3,1) )\n",
    "def inter(f1, f2):\n",
    "    show_decision_boundary(f1,f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you add interactive sliders to function **show_decision_boundary**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a plot similar to that of **show_decision_boundary**, but in which just the **test** data is shown.\n",
    "Look back at your answer to *Fast exercise 1*. Is it corroborated by your plot? Are the errors clearly visible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_decision_boundary2(f1,f2):\n",
    "    # Fit Gaussian to each class\n",
    "    ### UPDATE CODE HERE ###\n",
    "    features = [f1, f2]\n",
    "    mu, covar, pi = fit_generative_model(trainx, trainy, features)\n",
    "    \n",
    "    # Set up dimensions of plot\n",
    "    x1_lower, x1_upper = find_range(testx[:,f1])\n",
    "    x2_lower, x2_upper = find_range(testx[:,f2])\n",
    "    plt.xlim([x1_lower,x1_upper])\n",
    "    plt.ylim([x2_lower,x2_upper])\n",
    "\n",
    "    # Plot points in testing set\n",
    "    colors = ['r', 'k', 'g']\n",
    "    for label in range(0,3):\n",
    "        plt.plot(testx[testy==label,f1], testx[testy==label,f2], marker='o', ls='None', c=colors[label])\n",
    "\n",
    "    # Define a dense grid; every point in the grid will be classified according to the generative model\n",
    "    res = 200\n",
    "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
    "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
    "\n",
    "    # Declare random variables corresponding to each class density\n",
    "    random_vars = {}\n",
    "    for label in range(0,3):\n",
    "        random_vars[label] = multivariate_normal(mean=mu[label,:],cov=covar[label,:,:])\n",
    "\n",
    "    # Classify every point in the grid; these are stored in an array Z[]\n",
    "    Z = np.zeros((len(x1g), len(x2g)))\n",
    "    for i in range(0,len(x1g)):\n",
    "        for j in range(0,len(x2g)):\n",
    "            scores = []\n",
    "            for label in range(0,3):\n",
    "                scores.append(np.log(pi[label]) + random_vars[label].logpdf([x1g[i],x2g[j]]))\n",
    "            Z[i,j] = np.argmax(scores) + 1\n",
    "\n",
    "    # Plot the contour lines\n",
    "    plt.contour(x1g,x2g,Z.T,3,cmap='seismic')\n",
    "    \n",
    "    # Finally, show the image\n",
    "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
    "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cd0cd80c3840b2ae61aeb42beff9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='f1', max=3), IntSlider(value=1, description='f2', max=3)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual( f1=IntSlider(0,0,3,1), f2=IntSlider(1,0,3,1) )\n",
    "def inter2(f1, f2):\n",
    "    show_decision_boundary2(f1, f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
