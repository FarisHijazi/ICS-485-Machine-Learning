{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying IRIS species using univariate Gaussian Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can use built-in code for mean, variance, covariance, determinant, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard includes\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Useful module for dealing with the Gaussian density\n",
    "from scipy.stats import norm, multivariate_normal #in case you use buit-in library\n",
    "# installing packages for interactive graphs\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "featurenames = ['petal_length', 'petal_width', 'sepal_length', 'sepal_width']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 150 instances into training set (trainx, trainy) of size 105 and test set (testx, testy) of size 45\n",
    "np.random.seed(0)\n",
    "perm = np.random.permutation(150)\n",
    "trainx = X[perm[0:105],:]\n",
    "trainy = Y[perm[0:105]]\n",
    "testx = X[perm[105:150],:]\n",
    "testy = Y[perm[105:150]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many training points there are from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = sum(trainy==0)\n",
    "c1 = sum(trainy==1)\n",
    "c2 = sum(trainy==2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Can you figure out how many test points there are from each class? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 16, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: add your code to find how many test points there are from each class\n",
    "sum(testy == 0), sum(testy == 1), sum(testy == 2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the distribution of a single feature from one of the species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick just one feature: 'petal_length'. This is the first feature, that is, number 0. Here is a *histogram* of this feature's values under species 1, along with the *Gaussian fit* to this distribution.\n",
    "\n",
    "<img src=\"density.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a84b704dc84b1fa9a9e0cf85550e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='feature', max=3), IntSlider(value=0, description='label'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual( feature=IntSlider(0,0,3), label=IntSlider(0,0,2))\n",
    "def density_plot(feature, label):\n",
    "    plt.hist(trainx[trainy==label,feature], density=True)\n",
    "    #\n",
    "    mu = np.mean(trainx[trainy==label,feature]) # mean\n",
    "    var = np.var(trainx[trainy==label,feature]) # variance\n",
    "    std = np.sqrt(var) # standard deviation\n",
    "    x_axis = np.linspace(mu - 3*std, mu + 3*std, 1000)\n",
    "    plt.plot(x_axis, norm.pdf(x_axis,mu,std), 'r', lw=2)\n",
    "    plt.title(\"Species \"+str(label) )\n",
    "    plt.xlabel(featurenames[feature], fontsize=14, color='red')\n",
    "    plt.ylabel('Density', fontsize=14, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. In the function **density_plot**, the code for plotting the Gaussian density focuses on the region within 3 standard deviations of the mean. Do you see where this happens? Why do you think we make this choice?\n",
    "\n",
    "### Q3. Here's something for you to figure out: for which feature (0-3) does the distribution of (training set) values for species-2 have the *smallest* standard deviation? what is the value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this cell\n",
    "std = np.zeros(4)\n",
    "### START CODE HERE ###\n",
    "Class0 = np.zeros(shape = [c0, 4])\n",
    "Class1 = np.zeros(shape = [c1, 4])\n",
    "Class2 = np.zeros(shape = [c2, 4])\n",
    "\n",
    "s0 = 0\n",
    "s1 = 0\n",
    "s2 = 0\n",
    "for i in range(len(trainy)):\n",
    "    if trainy[i] == 0:\n",
    "        Class0[s0] = trainx[i]\n",
    "        s0 += 1\n",
    "    elif trainy[i] == 1:\n",
    "        Class1[s1] = trainx[i]\n",
    "        s1 += 1\n",
    "    elif trainy[i] == 2:\n",
    "        Class2[s2] = trainx[i]\n",
    "        s2 += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = Class2.std(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['petal_length', 'petal_width', 'sepal_length', 'sepal_width']\n",
      "[0.56659062 0.28784229 0.46420337 0.28097574]\n"
     ]
    }
   ],
   "source": [
    "print(featurenames)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fit a Gaussian to each class\n",
    "Let's define a function that will fit a Gaussian generative model to the three classes, restricted to just a <font color = 'red'>single feature</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_generative_model(x,y,feature):\n",
    "    mu = np.array([])\n",
    "    mu = np.append(mu, Class0.mean(axis = 0)[feature])\n",
    "    mu = np.append(mu, Class1.mean(axis = 0)[feature])\n",
    "    mu = np.append(mu, Class2.mean(axis = 0)[feature])\n",
    "\n",
    "    var = np.array([])\n",
    "    var = np.append(var, Class0.var(axis = 0)[feature])\n",
    "    var = np.append(var, Class1.var(axis = 0)[feature])\n",
    "    var = np.append(var, Class2.var(axis = 0)[feature])\n",
    "\n",
    "    pi = np.array([])\n",
    "    pi = np.append(pi, (np.sum(trainy == 0) / len(trainy)))\n",
    "    pi = np.append(pi, (np.sum(trainy == 1) / len(trainy)))\n",
    "    pi = np.append(pi, (np.sum(trainy == 2) / len(trainy)))\n",
    "    \n",
    "    return mu, var, pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call this function on the feature 'petal_length'. What are the class weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 0 # 'petal_length'\n",
    "### START CODE HERE ###\n",
    "mmm, vvv, ppp = fit_generative_model(trainx, trainy, feature)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.95757576 5.9        6.49473684]\n",
      "[0.13880624 0.26176471 0.32102493]\n",
      "[0.31428571 0.32380952 0.36190476]\n"
     ]
    }
   ],
   "source": [
    "print(mmm)\n",
    "print(vvv)\n",
    "print(ppp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, display the Gaussian distribution for each of the three classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f184abd0a8d14cc7bc91c440af757519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='feature', max=3), Button(description='Run Interact', sty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual( feature=IntSlider(0,0,3) )\n",
    "def show_densities(feature):\n",
    "    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "    colors = ['r', 'k', 'g']\n",
    "    for label in range(0,3):\n",
    "        m = mu[label]\n",
    "        s = np.sqrt(var[label])\n",
    "        x_axis = np.linspace(m - 3*s, m+3*s, 1000)\n",
    "        plt.plot(x_axis, norm.pdf(x_axis,m,s), colors[label], label=\"species-\" + str(label))\n",
    "    plt.xlabel(featurenames[feature], fontsize=14, color='red')\n",
    "    plt.ylabel('Density', fontsize=14, color='red')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "Use the widget above to look at the three class densities for each of the 4 features. Here are some questions for you:\n",
    "1. For which feature (0-3) do the densities for classes 0 and 2 *overlap* the most?\n",
    "2. For which feature (0-3) is class 2 the most spread out relative to the other two classes?\n",
    "3. For which feature (0-3) do the three classes seem the most *separated* (this is somewhat subjective at present)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well can we predict the class (0, 1, 2) based just on one feature? The code below lets us find this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.95757576 5.9        6.49473684]\n",
      "[0.13880624 0.26176471 0.32102493]\n",
      "[0.31428571 0.32380952 0.36190476]\n"
     ]
    }
   ],
   "source": [
    "mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "print(mu)\n",
    "print(var)\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalPdf(x, mu, var, pi):\n",
    "    contPart = 1/np.sqrt(2*np.pi*var)\n",
    "    expPart = np.exp((-np.power(x - mu, 2) / (2*var)))\n",
    "    px = pi * contPart * expPart\n",
    "    return px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20cf064d4c447359d6c7599fe24bbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='feature', max=3), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact( feature=IntSlider(0,0,3) )\n",
    "def test_model(feature):\n",
    "    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "    \n",
    "    k = 3 # Labels 0,1,2,...,k\n",
    "    n_test = len(testy) # Number of test points\n",
    "    score = np.zeros((n_test,k)) # here it was k + 1 and I earse the 1 (k+1)\n",
    "    for i in range(0,n_test):\n",
    "        for label in range(0,k):\n",
    "            ### START CODE HERE ###\n",
    "            # Implement the formula for normal pdf. \n",
    "            # If you can't, use the built-in norm.logpdf() but to get the full grades you should implement your own  \n",
    "            \n",
    "            score[i,label] = NormalPdf(testx[i][feature], mu[label], var[label], pi[label])\n",
    "    predictions = np.argmax(score, axis = 1) #think about using np.argmax on score[]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    # Finally, tally up score\n",
    "    errors = np.sum(predictions != testy)\n",
    "    print (\"Test error using feature \" + featurenames[feature] + \": \" + str(errors) + \"/\" + str(n_test))\n",
    "    print(\"Test Error using feature %s: %.2f%%\"%(featurenames[feature], (errors/n_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_pred(x, y, feature):\n",
    "    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "    \n",
    "    k = 3 # Labels 0,1,2,...,k\n",
    "    n_test = len(y) # Number of test points\n",
    "    score = np.zeros((n_test,k)) # here it was k + 1 and I earse the 1 (k+1)\n",
    "    for i in range(0,n_test):\n",
    "        for label in range(0,k):\n",
    "            ### START CODE HERE ###\n",
    "            # Implement the formula for normal pdf. \n",
    "            # If you can't, use the built-in norm.logpdf() but to get the full grades you should implement your own  \n",
    "            \n",
    "            score[i,label] = NormalPdf(x[i][feature], mu[label], var[label], pi[label])\n",
    "    predictions = np.argmax(score, axis = 1) #think about using np.argmax on score[]\n",
    "    errors = np.sum(predictions != y)\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "In this notebook, we are looking at classifiers that use just one out of a possible 4 features. Choosing a subset of features is called **feature selection**. In general, this is something we would need to do based solely on the *training set*--that is, without peeking at the *test set*.\n",
    "\n",
    "For the IRIS data, compute the training error and test error associated with each choice of feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "training error for feature #0 = 24.76%\n",
      "45\n",
      "training error for feature #1 = 42.86%\n",
      "4\n",
      "training error for feature #2 = 3.81%\n",
      "4\n",
      "training error for feature #3 = 3.81%\n",
      "\n",
      "12\n",
      "test error for feature #0 = 26.67%\n",
      "20\n",
      "test error for feature #1 = 44.44%\n",
      "3\n",
      "test error for feature #2 = 6.67%\n",
      "2\n",
      "test error for feature #3 = 4.44%\n"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "error = 0\n",
    "for i in range(testx.shape[1]):\n",
    "    error = test_model_pred(trainx, trainy, i)\n",
    "    print(error)\n",
    "    print(\"training error for feature #%d = %.2f%%\"%(i , 100*error/len(trainy)))\n",
    "\n",
    "\n",
    "    \n",
    "print()    \n",
    "for i in range(testx.shape[1]):\n",
    "    error = test_model_pred(testx, testy, i)\n",
    "    print(error)\n",
    "    print(\"test error for feature #%d = %.2f%%\"%(i , 100*error/len(testy)))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your findings, answer the following questions:\n",
    "* Which two features have the lowest training error? List them in order (best first).\n",
    "* Which two features have the lowest test error? List them in order (best first)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
